{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Code to analyze the cleaned homeless deaths data using PySpark\n",
    "Note: \n",
    "\n",
    "I ran this code on google colab, so it has the code for that, consider running it there rather than locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1BlqvDWMFNU"
   },
   "outputs": [],
   "source": [
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3-scala2.13.tgz\n",
    "!tar xf spark-3.5.1-bin-hadoop3-scala2.13.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "spark_version = 'spark-3.0.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "os.environ[\"JAVA_HOME\"] = f\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/spark-3.5.1-bin-hadoop3-scala2.13\""
   ],
   "metadata": {
    "id": "CbfCCupDMyd0"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Homeless Deaths Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_by_cause = spark.read.csv('/content/cleaned_homeless-deaths-by-cause.csv', header=True, inferSchema=True)\n",
    "data_by_demographics = spark.read.csv('/content/cleaned_homeless-deaths-by-demographics.csv', header=True, inferSchema=True)\n",
    "data_by_month = spark.read.csv('/content/cleaned_homeless-deaths-by-month.csv', header=True, inferSchema=True)\n",
    "\n",
    "filtered_data_by_cause = data_by_cause.filter(data_by_cause[\"Year of death\"] == 2023)\n",
    "aggregated_data_by_cause = data_by_cause.groupBy(\"Cause_of_death\").sum(\"Count\")\n",
    "\n",
    "filtered_data_by_cause.show()\n",
    "aggregated_data_by_cause.show()\n",
    "\n",
    "aggregated_data_by_cause.write.parquet('/content/aggregated_homeless_deaths_by_cause.parquet')\n",
    "\n",
    "spark.stop()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF6CpeJpp5cp",
    "outputId": "503db029-25ec-4de9-9614-6299e22e0a9f"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-------------+--------------------+---------+-------+-----+\n",
      "|_id|Year of death|      Cause_of_death|Age_group| Gender|Count|\n",
      "+---+-------------+--------------------+---------+-------+-----+\n",
      "|  1|         2023|cardiovascular di...|  Unknown|   Male|    1|\n",
      "| 16|         2023|       drug toxicity|    40-59| Female|    7|\n",
      "| 22|         2023|             suicide|    40-59|   Male|    1|\n",
      "| 35|         2023|           pneumonia|  Unknown|   Male|    1|\n",
      "| 36|         2023|             unknown|  Unknown|Unknown|    1|\n",
      "| 56|         2023|             unknown|  Unknown| Female|    2|\n",
      "| 65|         2023|             unknown|    20-39| Female|    3|\n",
      "| 72|         2023|               other|    20-39| Female|    1|\n",
      "| 79|         2023|       drug toxicity|    20-39|   Male|   16|\n",
      "| 90|         2023|            accident|    40-59|   Male|    4|\n",
      "|102|         2023|              cancer|    40-59|   Male|    2|\n",
      "|105|         2023|            accident|  Unknown|   Male|    1|\n",
      "|127|         2023|       drug toxicity|    40-59|   Male|   24|\n",
      "|139|         2023|       drug toxicity|    20-39| Female|    2|\n",
      "|141|         2023|             unknown|      60+|   Male|   20|\n",
      "|171|         2023|             unknown|      60+| Female|    2|\n",
      "|177|         2023|             unknown|  Unknown|   Male|    2|\n",
      "|180|         2023|cardiovascular di...|      60+|   Male|    5|\n",
      "|183|         2023|            homicide|    40-59|   Male|    2|\n",
      "|185|         2023|               other|      60+|   Male|    4|\n",
      "+---+-------------+--------------------+---------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+----------+\n",
      "|      Cause_of_death|sum(Count)|\n",
      "+--------------------+----------+\n",
      "|             suicide|        30|\n",
      "|            covid-19|         8|\n",
      "|             unknown|       225|\n",
      "|cardiovascular di...|       104|\n",
      "|               other|        50|\n",
      "|       drug toxicity|       457|\n",
      "|              cancer|        44|\n",
      "|            accident|        43|\n",
      "|           pneumonia|        23|\n",
      "|           infection|        10|\n",
      "|            homicide|        21|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Homeless Deaths Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_by_cause = spark.read.csv('/content/cleaned_homeless-deaths-by-cause.csv', header=True, inferSchema=True)\n",
    "data_by_demographics = spark.read.csv('/content/cleaned_homeless-deaths-by-demographics.csv', header=True, inferSchema=True)\n",
    "data_by_month = spark.read.csv('/content/cleaned_homeless-deaths-by-month.csv', header=True, inferSchema=True)\n",
    "\n",
    "# QoL Column renaming\n",
    "## Rename the column count to demo_count\n",
    "data_by_demographics = data_by_demographics.withColumnRenamed(\"count\", \"demo_count\")\n",
    "## Rename the column Age_group to demo_age_group\n",
    "data_by_demographics = data_by_demographics.withColumnRenamed(\"Age_group\", \"demo_age_group\")\n",
    "## Rename the column Gender to demo_gender\n",
    "data_by_demographics = data_by_demographics.withColumnRenamed(\"Gender\", \"demo_gender\")\n",
    "\n",
    "## Rename the column count to cause_count\n",
    "data_by_cause = data_by_cause.withColumnRenamed(\"count\", \"cause_count\")\n",
    "\n",
    "## Rename the column count to month_count\n",
    "data_by_month = data_by_month.withColumnRenamed(\"count\", \"month_count\")\n",
    "\n",
    "# Join data_by_cause with data_by_demographics on \"Year of death\"\n",
    "joined_cause_demographics = data_by_cause.join(\n",
    "    data_by_demographics,\n",
    "    on=[\"Year of death\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Join the result with data_by_month on \"Year of death\"\n",
    "joined_all = joined_cause_demographics.join(\n",
    "    data_by_month,\n",
    "    on=[\"Year of death\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Select relevant columns to avoid duplicates\n",
    "selected_columns = joined_all.select(\n",
    "    \"Year of death\",\n",
    "    \"Cause_of_death\",\n",
    "    \"Age_group\",\n",
    "    \"Gender\",\n",
    "    \"Month of death\",\n",
    "    joined_cause_demographics[\"cause_count\"].alias(\"Count_by_cause\"),\n",
    "    data_by_demographics[\"demo_count\"].alias(\"Count_by_demographics\"),\n",
    "    data_by_month[\"month_count\"].alias(\"Count_by_month\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "selected_columns.show()\n",
    "\n",
    "# Save the final DataFrame to Parquet\n",
    "# selected_columns.write.parquet('joined_homeless_deaths_data.parquet')\n",
    " \n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "emryvG3nrPNp",
    "outputId": "4c8c553c-b1e3-41e6-df75-92787aa1e056"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------------+---------+------+--------------+--------------+---------------------+--------------+\n",
      "|Year of death|      Cause_of_death|Age_group|Gender|Month of death|Count_by_cause|Count_by_demographics|Count_by_month|\n",
      "+-------------+--------------------+---------+------+--------------+--------------+---------------------+--------------+\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|     September|             1|                   24|            15|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|      December|             1|                   24|             9|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|        August|             1|                   24|             5|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|       January|             1|                   24|            19|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|       October|             1|                   24|            10|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|           May|             1|                   24|            14|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|          July|             1|                   24|            13|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|          June|             1|                   24|            13|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|      November|             1|                   24|            16|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|         March|             1|                   24|            14|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|         April|             1|                   24|             8|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|      February|             1|                   24|            13|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|     September|             1|                    6|            15|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|      December|             1|                    6|             9|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|        August|             1|                    6|             5|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|       January|             1|                    6|            19|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|       October|             1|                    6|            10|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|           May|             1|                    6|            14|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|          July|             1|                    6|            13|\n",
      "|         2023|cardiovascular di...|  Unknown|  Male|          June|             1|                    6|            13|\n",
      "+-------------+--------------------+---------+------+--------------+--------------+---------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/path/to/joined_homeless_deaths_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-ce8ddf2b9dec>\u001B[0m in \u001B[0;36m<cell line: 53>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;31m# Save the final DataFrame to Parquet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m \u001B[0mselected_columns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/path/to/joined_homeless_deaths_data.parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;31m# Stop the Spark session\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1719\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1720\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_opts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1721\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1722\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1723\u001B[0m     def text(\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1322\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1323\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1324\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    183\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [PATH_ALREADY_EXISTS] Path file:/path/to/joined_homeless_deaths_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ]
  }
 ]
}
